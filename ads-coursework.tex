\documentclass[11pt,a4paper]{article}

\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\lstset{
  breakatwhitespace=true,
  breaklines=true,
  frame=single
}

\colorlet{punct}{red!60!black}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\title{Advanced Database Systems Coursework}
\author{Ross Fenning}

\begin{document}

\maketitle

\section{Introduction}

The aim of this report is to explore and compare the different properties of
three different kinds of databases beyond the traditional relational model. In it,
the same dataset will be modelled in each of an \emph{Object-Relational} database,
an \emph{XML} database and a \emph{Document} database.

A suggested structure or schema will be designed for each database type, then
each design will be implemented in each of \emph{PostgreSQL}, \emph{eXist}
and \emph{MongoDB} along with a discussion around how each database would be
queried for different use cases.

Finally, there will be a comparitive discussion around the advantages and
disadvantages between the different database types, including which the use
cases to which each kind of database are most appropriate.

\section{The Data}

The dataset that will be used is a small set of 7,788 TV and radio programmes
that have been available to listen to or watch on some form of catch-up
service (e.g. the old radio ``Listen Again'' service or BBC iPlayer) between
2001 and 2012.

The raw data begin in a comma-separated format, with each row representing
a single programme. Each programme row comprises:

\begin{itemize}
    \item an identifier known as a \emph{pid} (programme ID);
    \item a start and end time (in both Unix timestamp and ISO 8601 formats);
    \item a title;
    \item a media type (i.e. audio or video);
    \item the master brand with which the programme is associated (somewhat
      correlated with the channel that broadcasted the programme, e.g.
      BBC Radio Four or BBC Two);
    \item the service that did broadcast the programme originally;
    \item a pid identifier for the programme's brand (e.g. for an episode of
      Doctor Who this would be the identifier for the Doctor Who concept
      in general);
    \item whether or not this is a programme or a just a clip that was
      available to watch/listen to;
    \item any categories associated with the programme; and
    \item any tags set against the programme.
\end{itemize}

We might imagine that this will form the basis of a whole archive of BBC
TV and radio programmes with a variety of use cases for such an archive.
Potential uses for such a database will be discussed as different designs
and approaches are used to import this information into three different
database systems.

\section{Object-Relational}
\label{sec:postgres}

The Object-Relational database system used here is PostgreSQL, which supports
pure relational databases and SQL, but also has support for objects and
inheritance to support an aggregate-oriented approach, where we can store
aggregates of data together as single objects for atomic updates against
them or simply to group information together into logical objects. This
takes us somewhat away from the pure relational model where data aggregation
is distributed across relations between many tuples. \cite{sadalage2012nosql}

\subsection{Design}

We can create a very na\"ive design that simply treats each row in the
CSV file as a single relation. This is shown in figure~\ref{fig:single-relation}.
This is clearly not even in first normal form \cite{codd1972further}
and a purely relational database
is not going to handle the list of tags nor the complex category structures.

\subsubsection{Pure Relational Model}

\begin{comment}
  @startuml plant_single_relation.png
  class Programme {
    pid
    start_time
    end_time
    epoch_start
    epoch_end
    complete_title
    media_type
    masterbrand
    service
    brand_pid
    is_clip
    categories
    tags
  }
  @enduml
\end{comment}
\begin{figure}[p]
  \begin{center}
    \includegraphics[width=1in]{plant_single_relation.png}
  \end{center}
  \caption{Representing the dataset as a single relation}
  \label{fig:single-relation}
\end{figure}

An obvious next step is to create relations for categories and for tags,
with relations. This would produce something similar to
Figure~\ref{fig:cat-tags-rel}.

\begin{comment}
  @startuml plant_three_relations.png
  class Programme {
    pid
    start_time
    end_time
    epoch_start
    epoch_end
    complete_title
    media_type
    masterbrand
    service
    brand_pid
    is_clip
  }
  class Category {
    id
    level
    title
  }
  class Tag {
    title
  }
  Programme "0..*" -r- "0..*" Category
  Programme "0..*" -l- "0..*" Tag
  @enduml
\end{comment}
\begin{figure}[p]
  \begin{center}
    \includegraphics[width=\linewidth]{plant_three_relations.png}
  \end{center}
  \caption{Relations created for the list/set types}
  \label{fig:cat-tags-rel}
\end{figure}

We can do a lot more normalisation and achieve the model shown in
figure~\ref{fig:3nf}. Note also that the redundancy of having both
timestamps and date strings in ISO 8601 format is removed.

\begin{comment}
  @startuml plant_3nf.png
  class Programme {
    pid : varchar
    complete_title : varchar
    media_type : enum ('video, 'audio')
    masterbrand : varchar
    brand_pid : varchar
    is_clip : boolean
  }
  class AvailabilityWindow {
    start_time : timestamp
    end_time : timestamp
    service : varchar
  }
  class Category {
    id : varchar
    level : smallint
    title : varchar
  }
  class Tag {
    title : varchar
  }
  Programme "1" -d- "0..*" AvailabilityWindow
  Programme "0..*" -r- "0..*" Category
  Programme "0..*" -l- "0..*" Tag
  @enduml
\end{comment}
\begin{figure}[p]
  \begin{center}
    \includegraphics[width=\linewidth]{plant_3nf.png}
  \end{center}
  \caption{Removed date redundancy allows the model to be in 3NF}
  \label{fig:3nf}
\end{figure}

When we come to implement the entities and their respective relations
as shown in figure~\ref{fig:3nf}, we would create the following relations:

\begin{enumerate}
  \item Programme(pid, complete\_title, media\_type, masterbrand, brand\_pid, is\_clip)
  \item AvailabilityWindow(pid, start\_time, end\_time, service)
  \item Category(id, level, title)
  \item ProgrammeCategory(pid, category\_id)
  \item Tag(title)
  \item ProgrammeTag(pid, title)
\end{enumerate}

So, what benefits can we can from the non-SQL or object-like features of
PostgreSQL over and above the pure relational mode?

\subsubsection{Aggregate model and array fields}

If we wanted to search for a set of programmes along with their tags, we would
have to do a JOIN query each time. With very large Programme and Tag tables, this
JOIN could get very expensive -- especially with complex filtering involved.

In a more aggregate-oriented approach, we could treat the tags as an
intrinsic part of a programme \emph{aggregate} -- we would only want to update
the tags within the context of updating a programme and we're hardly
ever likely to query a Tag relation independently of the Programme relation.
With this, we could employ an array type within PostgreSQL with either the
arrays pointing to a table of all potential Tag values
(using \emph{ELEMENT REFERENCES})
or the arrays containing string values of the tags themselves. With string
arrays, we lose the consistency of being able to update a tag itself (say,
for spelling corrections), but we have a far faster way of changing the tags
of a programme whilst updating the programme aggregate as a whole.

Note that updating the aggregate atomically in this way removes the need
to use transactions, which would be necessary if we were both updating the
Tag and Programme relations in a single update. Again, we could see some
real performance gains if this database were receiving a lot of updates and
would otherwise be subject to a lot of locking and contention. \cite{sadalage2012nosql}

The consistency then arguably becomes easier to maintain were we to scale
to a distributed database model. If a programme is modelled as an aggregate
containing its own tags, then an atomic update of a programme and its tags
can be replicated across nodes eventually without worry that the update
to the programme may synchronise before the update containg the new tag it's
supposed to have gained. In the pure relational model, we would have used
transactions to keep the two updates together; with the aggregate model,
that need is removed.

The key to a lot of this debate around the tags for a programme really lies
in how the business intends for that information to be used. Generally,
the concept of tagging is a loose way to add textual metadata to a
business object to aid searches, automatic aggregation and even data
mining potential (e.g. this user likes things with this particular tag,
so we can suggest them more content with similar tags).

Tagging is something that can even somewhat benefit from being unconstrained
and freely chosen as the item is added or updated. \cite{smith2008tagging}
It is also arguable that the harm in a tag being spelled incorrectly
on one programme is a minor harm (given it is additional metadata that
does not harm normal navigation nor basic application functions) when
compared to the benefit given to editors and content producers that
they can add as many tags freely as they please to their content items.

\subsubsection{Modelling programme categories}

Can we apply the same reasoning to the categories? Similarly to the
tags, we could consider adding them as an array type within the
Programme relation. Given they are more complex entities, we could
not easily store them as primitive strings, but the possibility of an
array of foreign key references to the Category relation is there. Another
option is that of an array of a custom \emph{composite} type.

With a composite type, we could create a category type and use that within
an array in our Programme relation. However, categories are likely to be
used very differently to tags in that they are commonly used to
drive hierarchical navigation in a user interface, allowing users
to browse categories of interest for the latest or most popular items
therewithin.

Given that we might (almost) equally query for programmes or categories,
we probably still want to keep their respective relations and rely on
JOIN queries to link the two when needed. It is not unreasonable to
expect an application where the categories themselves are rarely changed,
but programmes are individually added in frequent updates against these
relatively static categories. Thus we would have rare need for rapid,
expensive transactions where both tables need updating atomically. In
the rare cases where a category is added along with a programme,
we could easily pay the performance cost on such a one-off update.

The option of a composite type should be kept for consideration (perhaps
if further requirements gathering indicates it is a better approach),
but with the assumptions made so far, there seems to be little
case for it.

Another question is whether to model the hierarchical nature of the
category entity. The level attribute of a category (so far only values
\emph{1} and \emph{2} are observed) hints at a category hierarchy with
level 1 categories being root categories and level 2 categories being
children thereof. It is possible to model this using table
\emph{inheritence} within PostgreSQL. In this case, we could define
a Subcategory table that inherits the identifier and title attributes
from the Category table, but additionally adds a reference to any
other (sub)category indicating its parent.

Taking the approach of making tags string arrays within the programme
aggregate (for simplicity, performance and atomic updates) and using
inheritence to maintain a category hierarchy, we arrive at a final
candidate entity relationship design as shown in figure~\ref{fig:final_postgres}.

\begin{comment}
  @startuml plant_final_postgres.png
  class Programme {
    pid : varchar
    complete_title : varchar
    media_type : enum ['video, 'audio']
    masterbrand : varchar
    brand_pid : varchar
    is_clip : boolean
    tags : varchar[]
  }
  class AvailabilityWindow {
    start_time : timestamp
    end_time : timestamp
    service : varchar
  }
  class Category {
    id : varchar
    title : varchar
  }
  class Subcategory extends Category {
    parent : Category
  }
  Programme "1" -d- "0..*" AvailabilityWindow
  Programme "0..*" -r- "0..*" Category
  @enduml
\end{comment}
\begin{figure}[p]
  \begin{center}
    \includegraphics[width=\linewidth]{plant_final_postgres.png}
  \end{center}
  \caption{Candidate final design for PostgresSQL database}
  \label{fig:final_postgres}
\end{figure}

\subsection{Implementation}

With a design finalised, we can build a PostgreSQL database from
our data. A potential script that will create tables in the PostgreSQL
dialect of SQL is shown in figure~\ref{fig:create_table}.

\begin{figure}[p]
  \lstinputlisting[frame=single,language=SQL]{postgres_broken.sql}
  \caption{SQL script to create PostgreSQL tables for programme data}
  \label{fig:create_table}
\end{figure}

However, there is a problem with this schema. When we try to import the
data into these relations, PostgreSQL responds with an error:

\begin{verbatim}
ERROR:  insert or update on table "programme_category" violates\
 foreign key constraint "programme_category_category_id_fkey"
DETAIL:  Key (category_id)=(9200041) is not present in table \
"category".
\end{verbatim}

It seems that we are attempting to insert a link between a programme and
a category that doesn't exist. The ID 9200041 refers to a subcategory called
\emph{arts\_culture\_and\_the\_media}, underneath the \emph{factual} top-level
category. The problem we have encountered here is a limitation with the
inheritance model in PostgreSQL in that our foreign key from the
programme\_category relation to the category relation can only refer to things
that are strictly categories and not the subcategories. The child relation
of subcategory cannot inherit being a suitable reference from another table.
A similar limitation is seen again in that we have to define an explicit
primary key in the subcategory table as that is something else which
cannot inherit from a parent table.

The PostgreSQL documentation \cite{postgres-docs} confirms this to be the
case and moreover states ``There is no good workaround for this case.'' So,
we have to look at restructuring our schema a little as it appears that
the inheritance model in PostgreSQL is not as complete or pure as would
be expected from Object-Oriented Programming (OOP). In OOP, one important
principle is the Liskov Substitution Principle (LSP) that states -- at
a basic level -- that if some type $S$ is a subtype of a type $T$, then
any object or instance of $T$ may be swapped out for an object of
type $S$ without altering the behaviour of a programme or system.

PostgreSQL's inheritance model violates the Liskov Substituion Principle
in that our relation that refers to a category cannot have its foreign key
constrained field make a reference to the subcategory relation. In other
words, we are unable to use a subcategory logically everywhere we could
have used a category.

\begin{figure}[p]
  \lstinputlisting[frame=single,language=SQL,firstline=10,lastline=46]{postgres.sql}
  \caption{PostgreSQL script without using inheritance}
  \label{fig:flatten-category}
\end{figure}

There are a number of ways we could restructure our schema design to
avoid this limitation and a simple one is to eschew the inheritance feature
altogether and return to the ``old way'' -- a single category relation
with a nullable parent field (where a null parent indicates a top-level
category). This alteration is shown in figure~\ref{fig:flatten-category}.
This is a step back away from the object-relational model to the pure
relational model, but it seems the inheritance feature just was not
suitable for our needs in this case, but it may prove useful in other
situations.

\subsection{Querying}

With a schema design that now works and the CSV of programmes information
converted and imported into that schema, we can look at how we get
the information back out. Without formal requirements, we can only
suppose some of the use cases, but we can assume some relatively typical
needs. We can express these as questions as asked by an end user and then
look into the queries they would then translate into for the database.

We will also look at use cases where content editors may wish to update
or alter the information.

\subsubsection{What can I watch right now?}

This is a basic query where we seek everything where the start\_time is
in the past, but the end\_time is in the future:

\begin{lstlisting}[language=SQL]
  SELECT * FROM availability_window
    JOIN programme ON availability_window.pid = programme.pid
  WHERE start_time <= now() AND end_time >= now();
\end{lstlisting}

We should be able to trust the query optimisation to know to
narrow the availability\_window down to the relatively small set
of things available only now before doing the join to the larger
programme table.

\subsubsection{What categories are there?}

An application like BBC iPlayer may wish to query for a list of all
categories so as to create a navigation menu that allows users
to drill down into a particular category to see what is
available thereunder.

\begin{lstlisting}[language=SQL]
  SELECT * FROM category
  WHERE parent IS NULL
  ORDER BY title;
\end{lstlisting}

Note that we have to match on null parents to select only
top-level categories (we don't want to bombard a user with every
possible subcategory until they choose to drill down futher). Had
we been able to stick with our inheritance model that turned out
not to be possible, we could have had a arguably more elegant
query:

\begin{lstlisting}[language=SQL]
  SELECT * FROM ONLY category
  ORDER BY title;
\end{lstlisting}

Where the \emph{ONLY} keyword ensures we only return things that
are strictly categories and not from the subcategory child table.
This is arguably clearer as could be seen to follow the
\emph{Principle of Least Astonishment}
\cite{saltzer2009principles}
in that a programmer
is not having to be aware of explicitly adding ``parent IS NULL''
to a simple query for main categories only. It fits better with
the mental model of programmers familiar with object-oriented
programming to query for things of a given type than to filter
based on a field that a top-level category does not even have
at a semantic level. However, we are happy here to trade this
off if PostgreSQL's inheritance model does not support our needs.

\subsubsection{What subcategories are under my chosen category?}

We can query the category table to find, say, all subcategories under
the children's category:

\begin{lstlisting}[language=SQL]
  SELECT * FROM category
  WHERE parent = '9100001'
  ORDER BY title;
\end{lstlisting}

Again, we could have had a slightly clearer query with the inheritance
model:

\begin{lstlisting}[language=SQL]
  SELECT * FROM subcategory
  WHERE parent = '9100001'
  ORDER BY title;
\end{lstlisting}

Where it is arguably clearer we are specifically looking for a subcategory.
This is demonstrated further because we would be unable to do an accidental
attempt to find a top-level category with a parent:

\begin{lstlisting}[language=SQL]
  SELECT * FROM category
  WHERE parent = '9100001'
  ORDER BY title;
\end{lstlisting}

With the inheritance model, the query above would fail as the category
table has no parent field (only subcategories have parents), which
could be seen to be more robust against programmer error.

\subsubsection{What can I watch now within a single category?}

If our users have navigated to their favourite category, we probably want
to be able to query for everything that is available to watch right
now within that category only:

\begin{lstlisting}[language=SQL]
  SELECT * FROM availability_window
    JOIN programme ON availability_window.pid = programme.pid
    JOIN programme_category
      ON programme_category.pid = programme.pid
  WHERE start_time <= now() AND end_time >= now()
    AND programme_category.category_id = '9100001';
\end{lstlisting}

Note that we have no need to join on the whole category relation as the
navigation should have given us the category ID, which we can filter
sufficiently using only the programme\_category relation.

\subsubsection{What's become available to watch for a particular TV/Radio channel?}

This is a simple additional filter on the availability\_window relation:

\begin{lstlisting}[language=SQL]
  SELECT * FROM availability_window
    JOIN programme ON availability_window.pid = programme.pid
  WHERE start_time <= now() AND end_time >= now()
    AND service = 'bbc_one';
\end{lstlisting}

\subsubsection{What can I watch that has a given tag?}

This requires that we query within the tags array field:

\begin{lstlisting}[language=SQL]
  SELECT * FROM availability_window
    JOIN programme ON availability_window.pid = programme.pid
  WHERE start_time <= now() AND end_time >= now()
    AND 'history' = ANY(tags);
\end{lstlisting}

\subsubsection{I want to add a new programme}

Adding a new programme should be a simple, atomic insert into the programme
table:

\begin{lstlisting}[language=SQL]
  INSERT INTO programme
  (masterbrand, complete_title, pid, media_type, brand_pid, is_clip)
  VALUES
  ('cbbc', 'chucklevision:_series_18:_barryella', 'b0079214', 'video', 'b006w487', '0');
\end{lstlisting}

Fortunately, there is little inconsistency from simply adding links
to any existing categories in a subsequent update:

\begin{lstlisting}[language=SQL]
  INSERT INTO programme_category
  (pid, category_id)
  VALUES
  ('b0079214', '9100001');
\end{lstlisting}

It would not appear to be very harmful to have a programme in the database
that does not yet have categories assigned (users would simply not be able to
navigate to the programme easily until then) and there is no problem
with referential integrity between those two updates. If, however,
this situation is not acceptable even in a short time frame, we can bundle
these updates within a single transaction. It should be discussed in the
software requirements whether it is necessary to complicate the updates
if there is little risk of consistency problems. It is reasonable that
we would want our software to abandon the whole update if a content
editor tries to add a programme to a non-existent category. Again,
it is arguably more in line with the principle of least astonishment to
fail to add any programme information if there is a mistake in the transaction.

\subsubsection{I want to add a new programme with new tags}

It is trivial to include as many tags as we want in our original programme
insertion:

\begin{lstlisting}[language=SQL]
  INSERT INTO programme
  (tags, masterbrand, complete_title, pid, media_type, brand_pid, is_clip)
  VALUES
  ('{"comedy", "silly"}', 'cbbc', 'chucklevision:_series_18:_barryella', 'b0079214', 'video', 'b006w487', '0');
\end{lstlisting}

Due to the earlier decision to create the tags field as a denormalised, free-form
array of character strings against each programme, tagging a new programme
with new or existing tags is exactly the same operation. Our update is unaffected
by whether the tag is already present in the database or not. It is another
question for the software requirements gathering as to whether it is
acceptable that content editors can simply create tags in this loose manner (especially
if they are fully capable of creating tags with spelling mistakes).

\subsubsection{I want to add a new programme with a new category}

This is equivalent to inserting a new programme against an existing category,
except we must first do the update to insert the new category:

\begin{lstlisting}[language=SQL]
  INSERT INTO category (id, title) VALUES ('9100010', 'sport');
\end{lstlisting}

Again, there is a design decision to be made as to whether it is
acceptable for the category to be inserted with the programme update
then failing (and thus leaving the database with a category with no
programmes therewithin) or whether it is preferable to do all
three updates as a single transaction.

\subsubsection{I want to correct a spelling mistake in a title}

This is a simple, atomic update:

\begin{lstlisting}[language=SQL]
  UPDATE programme
  SET complete_title = 'new title'
  WHERE pid = 'abc123';
\end{lstlisting}

\subsubsection{I want to revoke a programme}

There may be a case where a planned broadcast of a programme is
cancelled for technical or legal reasons. If we have been populating
our on-demand/catch-up service database with the availability
window suggesting that users will be able to watch that programme soon,
we will want to be able to remove a programme's record and associated
availability.

In a more simpler case where we just want to remove the availability of
the programme to watch, we just need to remove its availability window
(if we lose the rights to let people watch the programme on-demand):

\begin{lstlisting}[language=SQL]
  DELETE FROM availability_window
  WHERE pid = 'abc123';
\end{lstlisting}

This deletion becomes less atomic if we need to cancel the programme
altogether (i.e. remove its existence altogether as it's not
even going to be broadcast on TV):

\begin{lstlisting}[language=SQL]
  DELETE FROM programme
  WHERE pid = 'abc123';
\end{lstlisting}

This deletion will break foreign key constraints unless we either
include a deletion of the availability window first or use
\emph{ON CASCADE DELETE} with our foreign key reference to ensure
that programme deletions always propagate to the
availability\_window table as well.

\section{XML Database}

In this section, we will explore importing the same programmes dataset
into an XML database. We will use \emph{eXist} as an example of such
a database system and step through the same design-implement-query
approach as used for PostgresSQL in section~\ref{sec:postgres}.

\subsection{Design}

Some of the discussion around the data model from section~\ref{sec:postgres}
will apply also as we look at a design for an XML database, but
in this section we can address any differences in the design approach.

In the initial stages, individual \emph{entities} were identified in the
dataset by stepping through the normalisation process methodically. It
is still valid that the entities we care about are \emph{programmes},
\emph{tags}, \emph{categories} (and \emph{subcategories}) and
\emph{availability windows}, but with XML we choose to structure these
very differently to the pure relation or the object-relational models.

With an XML database such as \emph{eXist}, schema design is not strictly
a barrier to being able to import information. So long as the data can
be expressed in valid XML, we are able to import it given the tendency
for XML databases to be intrinsically schemaless. Whilst this allows
greater flexibility for what we choose to store, it can be difficult
to query a database where all the documents have vastly different structures.
Simple things like whether a field is called ``title'' or ``heading''
can add unneeded complexity to querying applications that have to check
for multiple possible fields.

There is some value to keeping a consistent structure where possible
and the benefit of using XML in our data store is that we get all
the tooling and processes built for XML for free, including Xml Schema for
validating documents before we import them. We will thus look at creating
an XML Schema document (XSD) as part of the design phase, which we can
use to validate document as we import them.

We can use \lstinline|<xs:complexType>| to define a grouping of elements
in a way that is analogous to defining a relation in a purely relational
schema. In fact, an XML node and all its descendent nodes are actually an
example of the \emph{aggregate model} in that a node tree defines an
atomic entity with as much structure and complexity within that
aggregate as necessary (i.e. it does not have to be a flat relation in
normal form). In this way, we could design in such as way as to have
a complexType defining an XML subtree for each object-relational aggregate
we defined for PostgreSQL.

Following the principle of a complexType per entity, we could create
a schema like that shown in figure~\ref{fig:xsd}.

\begin{figure}[p]
  \lstinputlisting[frame=single,language=XML,basicstyle=\scriptsize]{programmes.xsd}
  \caption{XSD defining a complexType for each of our aggregates}
  \label{fig:xsd}
\end{figure}

We have a simpleType which parallels the custom enum type used in the
PostgreSQL database as well as a complexType per entity in the entity
relational diagram shown in figure~\ref{fig:final_postgres}. There is one
exception in that subcategories have not been modelled as a separate
type. With XML we can use its hierachical, nested nature to compose
a tree of categories (in that each \lstinline|<category>| tag may
itself have its own \lstinline|<categories>| tag). Query languages such
as Xpath and Xquery should permit us to search for top-level categories
only, subcategories only and even across all categories and subcategories
together (since their respective XML subtrees share the same complexType and
are thus isomorphic). These three possibilities are what we would want
from an inheritance model, so this nested composition could be said
to be equivalent to subcategories inheriting from a parent category type.

One important thing to note is that we have now denormalised the
categories and replicated them across each programme. This has
added a lot of redundancy, but is a simpler approach that makes better
use of the hiearchical nature of XML, whereas references and foreign
keys are not natively possible with XML documents. We will see how
simple this keeps the queries when we come to pull information back
out of the database.

\subsection{Implementation}

We can convert the data from the original CSV into multiple XML documents
similar to the example shown in figure~\ref{fig:xml-example} and then import
into eXist with a single command:

\begin{lstlisting}
$ $EXIST_HOME/bin/client.sh -m /db/programmes -p *.xml
\end{lstlisting}

\begin{figure}[p]
  \lstinputlisting[
  frame=single,language=XML,basicstyle=\scriptsize,breakatwhitespace=false
  ]{
    xml/b00sn5cy.xml
  }
  \caption{Example XML representation of a programme}
  \label{fig:xml-example}
\end{figure}

\subsection{Querying}

In this section, we see how to query the XML collection using Xquery to
achieve the same queries as performed in PostgreSQL.

\subsubsection{What can I watch right now?}

Rather than the join operator used in SQL, we can search amongst
the nested availability windows using XQuery:

\begin{lstlisting}
xquery version "3.0";
for $programme in //programme
where $programme/availability/window/start_time < current-dateTime()
  and $programme/availability/window/end_time > current-dateTime()
return $programme
\end{lstlisting}

\subsubsection{What categories are there?}

This is where the denormalisation of the categories becomes a difficulty.
In order to find all unique top-level categories, we have to query all
category tags across all programmes and aggregate them down to unique
categories only:

\begin{lstlisting}
xquery version "3.0";
let $all_categories := //programme/categories/category
for $category_id in distinct-values($all_categories/id)
let $categories := $all_categories[id=$category_id]
let $category_title := $categories[1]/title/text()
order by $category_title
return 
    <category>
        <id>{$category_id}</id>
        <title>{$category_title}</title>
    </category>
\end{lstlisting}

In this case, we may have wished to create an XML collection for just
category XML documents to aid quick retrieval of all categories. However,
it becomes less clear how to maintain some referential integrity
between programmes and their respective categories.

\subsubsection{What subcategories are under my chosen category?}

This is similar to the previous query:

\begin{lstlisting}
xquery version "3.0";

let $all_subcategories := //programme/categories/category[id="9100098"]/categories/category
for $subcategory_id in distinct-values($all_subcategories/id)
let $subcategories := $all_subcategories[id=$subcategory_id]
let $subcategory_title := $subcategories[1]/title/text()
order by $subcategory_title
return 
    <category>
        <id>{$subcategory_id}</id>
        <title>{$subcategory_title}</title>
    </category>
\end{lstlisting}

The advantage with the nested XML design is that we can alter the XPath at
the top of the query to search all categories whether they are subcategories
or not and we can easily scale to arbitrary levels of category nesting.

\subsubsection{What can I watch now within a single category?}

We can query for all available programmes within a given category or even subcategory:

\begin{lstlisting}
xquery version "3.0";
for $programme in //programme[descendant::category/id='9200010']
where $programme/availability/window/start_time < current-dateTime()
  and $programme/availability/window/end_time > current-dateTime()
return $programme
\end{lstlisting}

\subsubsection{What's become available to watch for a particular TV/Radio channel?}

We can alter the first query to make sure we only match on availability of
programmes broadcast originally on BBC One:

\begin{lstlisting}
xquery version "3.0";
for $programme in /programme
where $programme/availability/window[service='bbc_one']/start_time < current-dateTime()
  and $programme/availability/window[service='bbc_one']/end_time > current-dateTime()
return $programme
\end{lstlisting}

This turns out to be a very inefficient query without indexes. We should ensure
that our eXist DB collection is indexing on the service field.

\subsubsection{What can I watch that has a given tag?}

We can search for available programmes with a given tag:

\begin{lstlisting}
xquery version "3.0";
for $programme in //programme
where $programme/tags/tag/text() = 'science'
  and $programme/availability/window/start_time < current-dateTime()
  and $programme/availability/window/end_time > current-dateTime()
return $programme
\end{lstlisting}

\subsubsection{I want to add a new programme}

With eXist-db, there are multiple ways to import whole new documents. One
simple way for us to add a new programme document to the collection
is to use the command line Java admin client:

\begin{lstlisting}
bin/client.sh -m /db/programmes -p /path/to/xml/files
\end{lstlisting}

Alternatively, we can use XQuery Update syntax to add the new document:

\begin{lstlisting}
xquery version "3.0";
declare namespace xmldb="http://exist-db.org/xquery/xmldb";
xmldb:store('/db/programmes', 'abc123.xml', '<programe>...</programme>')
\end{lstlisting}

\subsubsection{I want to add a new programme with new tags}

Given that are tags for a programme are contained within each programme
document, there is nothing preventing entirely new tags being included
in any document adding in the previous manner.

\subsubsection{I want to add a new programme with a new category}

Similarly to tags, categories are denormalised through all documents, so
we can use any of the available ways to add new documents to include
any categories -- new or existing -- in those documents.

\subsubsection{I want to correct a spelling mistake in a title}

We can remove a document and add a replacement if our client
software simple wants to send a full document (based on an editor
changing the information in a form and submitting it):

\begin{lstlisting}
xquery version "3.0";
declare namespace xmldb="http://exist-db.org/xquery/xmldb";
let $file-name := 'abc123.xml'
let $remove-return-status := xmldb:remove('/db/programmes, $file-name)
xmldb:store('/db/programmes', $file-name, <programe>...</programme>)
\end{lstlisting}

Another way is to do a direct update on the document node itself:

\begin{lstlisting}
xquery version "3.0";
replace value of node fn:doc("abc123.xml")/programme/title
with 'New title'
\end{lstlisting}

\subsubsection{I want to revoke a programme}

\begin{lstlisting}
xquery version "3.0";
declare namespace xmldb="http://exist-db.org/xquery/xmldb";
let $file-name := 'abc123.xml'
xmldb:remove('/db/programmes, $file-name)
\end{lstlisting}

%$ <- makes emacs happy

\section{Document Database}

In this section, we will look at repeating the same design, implementation
and query steps for the document-oriented database \emph{MongoDB}. Whilst
XML databases are also normally considered also to be document-oriented,
systems like MongoDB take a different approach and have different use
cases.

\subsection{Design}

MongoDB is a NoSQL database that very much fits in with the idea of
an aggregate-oriented system whereby we think about creating objects
or aggregates that represent the items on which we will be wanting
to do atomic updates and other operations.

Following the design used for the XML documents, we can start with
JSON (although strictly MongoDB is a BSON data store, we can theoretically
treat JSON as equivalent for the purposes of design)
documents that follow the same principles as shown in figure~\ref{fig:big-json-doc}.

\begin{figure}[p]
  \begin{lstlisting}[language=json]
    {
      "pid": "abc123",
      "complete_title": "Programme Title Here",
      "media_type": "video",
      "masterbrand": "bbc_one",
      "is_clip": 0,
      "availability": [
      {
        "start_time": new Date("2012-08-20T05:32:00"),
        "end_time": new Date("2012-08-27T05:32:00"),
        "service": "bbc_two"
      }
      "categories": [
      {
        "id": "910003",
        "name": "drama",
        "categories": [
        {
          "id": "9200018",
          "name": "crime"
        }
        ]
      }
      ]
      ]
    }
  \end{lstlisting}
  \caption{JSON version of document structure as defined in XML earlier}
  \label{fig:big-json-doc}
\end{figure}

As usual, the JSON port of the XML document designed earlier is somewhat
terser and the information to markup ratio is favourable, but have we
lost some of the hierachical structure that the XML offered? With
XML databases, we can use XPath expressions to search for arbitrary
descendants of a node, but that does not fit in with how best
to model things in JSON/BSON and MongoDB.

\subsubsection{Modelling categories}
\label{sec:mongo-design-categories}

The official MongoDB documentation\cite{mongo-categories} suggests
a way to model categories that would lead to a programme category
structure as shown in figure~\ref{fig:json-categories}. References
to parent categories are shown using the provided ID strings, but
MongoDB's built-in \emph{\_id} field could equally be used.

\begin{figure}[p]
\begin{lstlisting}[language=json]
  [
    {
      "id": "9100003",
      "name": "drama"
    },
    {
      "id": "9200018",
      "name": "crime",
      "parent": "9100003",
      "ancestors": [
        {
          "id": "9100003",
          "name": "drama"
        }
      ]
    }
  ]
\end{lstlisting}
  \caption{Suggested method of modelling category hierarchies in MongoDB}
  \label{fig:json-categories}
\end{figure}

This approach to modelling categories appears to have several benefits.
Adding a new category, restucturing the hierarchy or correcting an
existing category document all become reasonably simple operations, which
we will hopefully see when we look at querying the database in
section~\ref{sec:mongo-querying}.

We have to be cautious, however, about the denormalisation of category
names. In the example shown in figure~\ref{fig:json-categories}, the
word ``drama'' appears not only in the \emph{drama} category document, but
also in the ancestor object within the \emph{crime} category document.
Should we want to rename that category, we have to ensure we update it
in all places. This can be done in only two update operations (one
for the category and one to update all ancestors at once), but
that still leaves open the risk of inconsistency if one succeeds, but
the other fails. Optionally, we could store only the IDs in the
ancestors property, but we would lose the convenience of a single
query being able to fetch a category and generate a ``bread crumb''
trail in one operation. In this case, we should return again
to specific software requirements as to whether we would want
our application being able to fetch a programme with its full category
bread crumb trail or whether we are happy for category hierarchies to
be reconstructed from IDs and follow-up queries.

It is this kind of denormalisation that is usually a strength of MongoDB.
The NoSQL approach of moving away from pure relations and normal forms
means that we can happily duplicate category names to support
both a single, atomic query for all categories but also a query
for a category and its bread crumb trail. This allows for client
applications to make very performant queries that retrieve everything
in single fetches, without having to think about join operations
or building ``views'' as would be the case in some SQL databases. This
feeds again into the aggregate-oriented approach where we think about
the aggregate objects that have business value and make sense for our
software to use instead of the mathematical purity of normalised,
relational tables. It could be argued as a pragmatic approach for
the right cases, but it clearly comes at a cost of consistency now
being a concern of the sofware, not the data store.

\subsubsection{Linking programmes and categories}

Following the suggested use case of a category hierarchy described
in section~\ref{sec:mongo-design-categories}, we have a design for
category objects in our database. How do we model a programme
being within a given category?

One approach is to have a categories collection alongside the
programmes collection. The categories field can then be a list
of ID references to appropriate categories within the categories
collection. This is closer to the relational model of having
separate tables for different entities, but with MongoDB such
references are not enforced as constraints in any way. It is
entirely possible to have dangling references to categories
that were removed.

If we accept that application code can take on the responsibility
of ensuring consistency, we can go further and create a
``programmes'' property on each category document with a list
of references to programmes within that category. Such a bidirectional
set of references \emph{must} by enforced by the client (or bespoke
middleware possibly using something like \emph{mongoengine}). We
should see in section~\ref{sec:mongo-querying} that this allows for
very powerful, single queries as long as the consistency trade-off
is acceptable.

\subsubsection{Availability}

Leaving categories aside, we know that it is likely a client
application will be wanting to query the programme model based
on whether it is available to watch at the current time. Given
that we are modelling the availability windows as a list/array
type within the programme object, this would mean looking inside
every array within every programme to find programmes where
at least one window matches. In order to make this query
less complex, we could create a collection of availability
windows with references to their respective programmes as shown
in figure~\ref{fig:mongo-availability-window}.

\begin{figure}[p]
\begin{lstlisting}[language=json]
  {
    "start_time": new Date("2012-08-20T05:32:00"),
    "end_time": new Date("2012-08-27T05:32:00"),
    "service": "bbc_two",
    "programme": "abc123"
  }
\end{lstlisting}
  \caption{Availability window modelled in MongoDB}
  \label{fig:mongo-availability-window}
\end{figure}

Modelling this as a separate collection is not only more
normalised (even though MongoDB will not enforce foreign key
constraints), but allows us to use some interesting features
of MongoDB and apply them only to the availability collection.

One feature that MongoDB provides is that of a TTL on documents
within a certain collection. \cite{mongo-expiry} With this
feature, we could well tell MongoDB to delete all availability
windows straight after the end time is reached. This has implications
of making sure the database is running on a server with accurate
time (assuming time zones do not add further problems), but the
benefit is that our queries for available programes do not even
have to include time comparisons. If we are assured that the
availability collection contains only windows for programmes
that are avaiable right now, we simply have to query the collection
for everything therin.

A foreseeable problem with having a separate availability
collection is that in the relational approach, we wanted to join
availability to programmes and again to categories so as to find
available programmes in a particular category -- something we
cannot do in MongoDB (it is not designed to do so). Instead,
the ``correct'' way to do this in line with MongoDB's ethos
is to denormalise further and start to embed some of a
programme's information within the availability window
object as show in figure~\ref{fig:mongo-availability-denormal}.
Note that the tags are also included as we may have a use
case for searching by tag.

\begin{figure}[p]
\begin{lstlisting}[language=json]
  {
    "start_time": new Date("2012-08-20T05:32:00"),
    "end_time": new Date("2012-08-27T05:32:00"),
    "service": "bbc_two",
    "programme": {
      "pid": "abc123",
      "categories": {"9100005": "factual", "9200041": "arts_culture_and_the_media"},
      "tags": ["history", "romans"]
    }
  }
\end{lstlisting}
  \caption{Availability window modelled with further denormalisation}
  \label{fig:mongo-availability-denormal}
\end{figure}

\subsubsection{Dates and times}

Note the use of Javascript date objects for start/end datetimes in our
programme document in figure~\ref{fig:big-json-doc}. This is
shown for illustration only as JSON lacks a definitive standard for date
and time objects. The BSON format used by MongoDB internally does
in fact support datetime fields, but this can be represented in a
number of ways if JSON is used as the transport format. The Javascript
date object notation is accepted by MongoDB in the shell, which
will be assumed to be the query client for our implementation and
querying, but other clients of MongoDB may choose very different
ways of representing dates and times. For example, the
Microsoft AJAX Library uses a special string format for dates in JSON:
``/Date(1234567890)/''. \cite{ms-ajax-json-dates} Other libraries
and clients may choose to write out the date as a string in ISO-8601 format,
but it is hard to dismbiguate that from a string literal containing
a date. It follows from the Principle of Least Astonishment again
that we might not want our software ``magically'' interpreting
string literals that happen to contain something that looks like a date
and time as a date/time object.

\subsection{Implementation}
\label{sec:mongo-implementation}

Implementation of a schemaless document store such as MongoDB is
incredibly trivial. This is due to removing all the constraints
of a relational database (with the obvious trade-offs). We can use
any language such as the Javascript used for the MongoDB shell to
start inserting objects without any prior setup. All we need to implement
our design is to install MongoDB and write our application code so that
it starts doing the correct update and select queries.

\subsection{Querying}
\label{sec:mongo-querying}

As stated in section~\ref{sec:mongo-implementation}, a MongoDB database
design manifests in the queries more than in any setup. We do not need
to design a schema ahead of time, but simply write our application such
that its select queries pull out information in the same format that
as the documents added via its insertion queries.

There are libraries and middleware such as \emph{mongoengine} (for
Python) and \emph{MongoMapper} (for Ruby) that allow a compromise
whereby a schema can be defined in a class structure, which is
then used by the application code. This is a good separation of concerns
with which the rest of the application is unable to insert or fetch
documents that do not meet expectations. However, the underlying
schemaless nature of MongoDB itself means it is easy to update
the schema that is defined in these model libraries and perhaps
certain operations can add additional fields to need later on. Effectively,
this allows applications to work to a rigid schema, but without the
database being the enforcer thereof.

For the queries to follow, we will assume raw queries with the MongoDB
shell without the benefits of any such middleware.

\subsubsection{What can I watch right now?}

With availability mapped in its own collection, we just need to query
for windows in that collection whose times fall either side of the
current time:

\begin{lstlisting}[language=json]
  var currentTime = new Date();
  db.availability.find({
    'start_time': {$lt: currentTime},
    'end_time': {$gt: currentTime}
  });
\end{lstlisting}

Of course, if we implement the TTL feature such that availability
window documents are removed automatically by MongoDB when the end time
is reached, we simply have to do:

\begin{lstlisting}[language=json]
  db.availability.find();
\end{lstlisting}

\subsubsection{What categories are there?}

If we want only top-level categories, we can query for all categories
without a parent:

\begin{lstlisting}[language=json]
  db.categories.find({"parent": {"$exists": false}});
\end{lstlisting}

\subsubsection{What subcategories are under my chosen category?}

If a user has navigated to a category and we want to list the available
subcategories, we can query for all categories that have that parent:

\begin{lstlisting}[language=json]
  db.categories.find({"parent": category_id});
\end{lstlisting}

\subsubsection{What can I watch now within a single category?}

Having denormalised the categories of a programme so that they appear
on availability windows as well, we can easily query for all
availability windows within any programme category:

\begin{lstlisting}[language=json]
  db.availability.find({"programme.categories.9100001": {"$exists": "true"}});
\end{lstlisting}

\subsubsection{What's become available to watch for a particular TV/Radio channel?}

This should be as easy as querying for availability windows, but with an
additional filter on service:

\begin{lstlisting}[language=json]
  db.availability.find({"service": "bbc_two"});
\end{lstlisting}

\subsubsection{What can I watch that has a given tag?}

To query amongst programmes with a given tag, we can just match our
tags value on the
array field and MongoDB will understand we are matching within
a list:

\begin{lstlisting}[language=json]
  db.availability.find({"programme.tags": "history"});
\end{lstlisting}

\subsubsection{I want to add a new programme}

When we add a new programme, we may want to add its availability
window as well. This is where we have to ensure consistency by
always doing two queries together:

\begin{lstlisting}[language=json]
  db.programmes.insert(
  {
    pid: "abc123",
    complete_title: "Programme Title Here",
    media_type: "video",
    "masterbrand": "bbc_one",
    "is_clip": 0,
    "categories": [
      {id: "9100005", name: "factual"),
      {id: "9200041", name: "arts_culture_and_the_media")
    ],
    "tags": ["history", "romans"]
  },
  );
  db.availability.insert(
  {
    "start_time": new Date("2012-08-20T05:32:00"),
    "end_time": new Date("2012-08-27T05:32:00"),
    "service": "bbc_two",
    "programme": {
      "pid": "abc123",
      "categories": {"9100005": "factual", "9200041": "arts_culture_and_the_media"},
      "tags": ["history", "romans"]
    }
  }
  );
\end{lstlisting}

This, of course, can be ensured in application code with a helper function
such as $addProgramme(programme_details, availability_details)$.

\subsubsection{I want to add a new programme with new tags}

Given that tags are modelled as string literals, this is no different to adding
a programme as above, but using string literals that aren't already in use.

\subsubsection{I want to add a new programme with a new category}

This is another case where we need multiple inserts and risk inconsistency.
Given that transactions do not fit into the way of doing things in MongoDB,
we should instead look at the application needs to ascertain the impact
of inconsistency. It is likely that it is less harmful for a programme to be
added under a category that is not yet navigable than it is for a category
to appear in the navigation that links to a programme that does not exist yet
(e.g. an HTTP 404 would be encountered in a web application). To this
end, we can add our programme before adding the new category:

\begin{lstlisting}[language=json]
  db.programmes.insert(
  {
    pid: "abc123",
    complete_title: "Programme Title Here",
    media_type: "video",
    "masterbrand": "bbc_one",
    "is_clip": 0,
    "categories": [
      {id: "9100005", name: "factual"),
      {id: "9200041", name: "arts_culture_and_the_media")
    ],
    "tags": ["history", "romans"]
  },
  );
  db.categories.insert(
    {
      "id": "9200041",
      "name": "arts_culture_and_the_media",
      "parent": "9100005",
      "ancestors": [
        {
          "id": "9100005",
          "name": "factual"
        }
      ],
      "programmes": [
        "abc123"
      ]
    }
  );
  db.availability.insert(
  {
    "start_time": new Date("2012-08-20T05:32:00"),
    "end_time": new Date("2012-08-27T05:32:00"),
    "service": "bbc_two",
    "programme": {
      "pid": "abc123",
      "categories": {"9100005": "factual", "9200041": "arts_culture_and_the_media"},
      "tags": ["history", "romans"]
    }
  }
  );
\end{lstlisting}

If we truly want this to be transactional, the MongoDB
documentation suggests \cite{mongo-two-phase-commits} that it
is best to do so with a ``state'' property on the documents.
Following this, we could put "state: pending" on the programme
whose category is not yet added, then update said property
to "committed" after the category is added. This is enforcing
consistency within the data model and we can ensure we only
search for programmes, categories, etc. that we know are in
a committed state. It is unlikely this level of control is needed
for a programmes catch-up service, however.

\subsubsection{I want to correct a spelling mistake in a title}

Updating a single programme document is fairly simple:

\begin{lstlisting}[language=json]
  db.programmes.update({pid: "abc123"},
    {$set: {"complete_title": "New Title"}}});
\end{lstlisting}

However, we need to remember that we have denormalised programme
information across the programmes and availability collections:

\begin{lstlisting}[language=json]
  db.programmes.update({pid: "abc123"},
    {$set: {"complete_title": "New Title"}}});
  db.availability.update({programme.pid: "abc123"},
    {$set: {"programme.complete_title": "New Title"}}},
    {multi: true});
\end{lstlisting}

\subsubsection{I want to revoke a programme}

We have to ensure we purge the programme from all collections:

\begin{lstlisting}[language=json]
  db.programmes.delete({pid: "abc123"});
  db.availability.delete({programme.pid: "abc123"});
  db.categories.update({},
    {$pull: "abc123"},
    {multi: true});
\end{lstlisting}

\section{Comparison}

A few of the advantages and drawbacks of each database system
and the respective design for our dataset have been discussed along
the way. In this section, we will summarise some of the comparison
points and discuss which of the three systems is strongest in each
area. As always, we would normally endeavour to collect more
substantial user requirements for the software applications
that intend to use the data store, so in many areas there
is likely no ``clear cut'' winner whilst we are speculating many
of these requirements.

\subsection{Ease of design and implementation}

\begin{itemize}
  \item RDBMS and SQL have familiarity amongst programmers.
  \item RDBMS suffer from object-relational impedance mismatch
  \item MongoDB documents are more similar to the object data
    structures used by programmers.
  \item MongoDB is capable of modelling even complex things
    when you embrace the denormalisation required, but it can
    feel like you are recreating features (e.g. foreign keys,
    tree structures) within the documents that come for free as
    part of other systems.
  \item The XML representation ``feels'' more natural for arranging
    as a hierarchy and searching descendants.
  \item Fetching XML from a database might appeal if the application
    is an XML-based API (XSLT and XSD can be used to transform
    and validate with need for a large amount of bespoke, imperative
    code).
\end{itemize}

This is perhaps the most pragmatic comparison for choosing which
database system to use, based not just on software requirements, but
also on the preferences and ability of the team building the applications.

MongoDB appears to offer the greatest flexibility to allow the developers
to start building the application and simply persist
the data structures that emerge in the application. In
a general software project with requirements not fully known, it might
be the best option so as to avoid tricky schema migrations as
the use cases are more fleshed out in time.

PostgreSQL and eXist are stronger contenders if there is greater
familiarity with SQL or XML or if other advantages thereof outweigh
the loose, flexible benefits of MongoDB.

\subsection{Performance}

\begin{itemize}
  \item MongoDB was developed primarily for rapid scaling, perhaps by
    trading off consistency.
  \item Almost any database can be performant if designed in the right
    way with the right indexes.
  \item MongoDB allows us to denormalise into different collections, e.g.
    to fetch a list of categories or a list of programmes.
  \item PostgreSQL allows us to fetch from either categories or programmes
    without trading off normalisation.
  \item With eXist, we have to decide whether a programme or a category
    is the top-level element and searching amongst descendants could get
    cumbersome.
\end{itemize}

This is a comparison that is brought up a lot in database discussions, but
could well be the least of a project's concerns. Clearly, MongoDB's focus
on denormalising information to everywhere it's needed and the
``fire and forget'' manner of updates by default allows for rapid storage
of a lot of information with a number of trade-offs to achieve that. Most
systems can be made to perform however and arguably more time should
be spent considering data integrity, consistency and other pragmatic aspects
rather than optimising prematurely.

\subsection{Data Integrity}

\begin{itemize}
  \item PostgreSQL and relational databases in general are certainly the
    most noted for maintaing data integrity with foreign keys, unique
    (and other) constraints and transaction support.
  \item PostgeSQL's object-relational style allows small amounts of
    integrity to be traded-off (e.g. storing all the tags as string
    arrays rather than enforce all programmes reference the same set).
  \item The drawback with relational databases is having to use transactions
    around updates to multiple tables when an atomic update to a single
    aggregate in XML or MongoDB's documents would be sufficient and perhaps
    more performant.
  \item MongoDB enforces zero to no data integrity, but there are
    application libraries that allow it to be enforced in the software
    itself.
  \item XML databases like eXist are strictly schemaless, but generally
    such systems support the XML Schema standard for validation.
  \item More recent standards like JSON schema could be employed for
    a quick and simple validation layer in an application persisting
    to MongoDB.
\end{itemize}

Clearly, PostgreSQL is a clear winner if we want absolute certainty
about the data staying in a consistent state. It is not impossible
to achieve some kind of schema or validation in the other two systems,
but sometimes these can hide the benefits NoSQL databases provide
by being not so rigid.

\subsection{Update Consistency}

\begin{itemize}
  \item Transactions in PostgreSQL can ensure no query sees
    ``half'' an update (e.g. a new category looking empty
    before a new programme is added to it).
  \item Document and XML stores like MongoDB and eXist can make
    transactions unnecessary by storing aggregate structures
    that represent the atomic updates the application is going to make.
  \item Once we start denormalising documents for better queries,
    we introduce the need for multi-step updates.
  \item It is down to requirements to decide the harm of a partial
    update being seen (e.g. is it all that harmful to have an empty
    category for up to a second before the programme is added to it?)
\end{itemize}

\subsection{Maturity and Community Support}

\begin{itemize}
  \item SQL databases now have decades of optimisation, development tool
    support and 3rd party libraries available. Their continuing
    popularity assures that there is much support in the community for any
    issues or learning.
  \item XML databases also benefit from many years of popularity around
    XML itself and the ecosystem around it (XSLT, XSD, etc.) Falling popularity
    in favour of JSON might mean the community support is also declining however.
  \item Furthermore, few XML databases are in active development compared
    with alternatives, which suggests the community support is not as
    pervasive.
  \item MongoDB is relatively new in comparison, but has matured enough
    to be in use on major production environments. \cite{mongo-production} It
    also has a large number of client APIs in multiple languages.
\end{itemize}

Maturity might be an issue like performance where there are more important
aspects to consider. The ``tried and tested'' feature of PostgreSQL might
give some assurance around performance, but that should not deter a
project from considering eXist or MongoDB if they have enough benefits to
outweigh it.

In modern software development, community support can be a very important
factor. If a system or application is relatively new, there is some strength
in there being a large, active community around it. It ensures that
any problems have been seen by a number of eyes and that any developers
we may hire are likely to have prior knowledge of the system.

\subsubsection{Conclusion}

The overall message is that choosing a database system comes down to
understanding your needs and there are few objective factors that
make one system entirely ``better'' than another. The other important
consideration is that a database forms only one part of a software
application architecture and too much focus on all the academic
differences between different systems might distract from progress
on the core functionality of the application.

Thust the questions we should be asking are: \emph{what do we need?}
and \emph{how much does it matter?}

\bibliographystyle{cell}
\bibliography{bibtex}
\end{document}
